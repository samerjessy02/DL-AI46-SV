{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b29692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 1) IMPORT LIBRARIES\n",
    "# ==========================================================\n",
    "\n",
    "# PyTorch core library\n",
    "import torch\n",
    "\n",
    "# Neural network module (contains Linear, ReLU, etc.)\n",
    "import torch.nn as nn\n",
    "\n",
    "# Optimization algorithms (SGD, Adam, etc.)\n",
    "import torch.optim as optim\n",
    "\n",
    "# Classical ML tools for dataset & splitting\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 2) LOAD DATASET\n",
    "# ==========================================================\n",
    "\n",
    "\"\"\"\n",
    "We use a classical ML1 dataset: Breast Cancer Wisconsin dataset.\n",
    "\n",
    "- 30 numerical features (radius, texture, perimeter, etc.)\n",
    "- Binary target (0 = malignant, 1 = benign)\n",
    "\n",
    "This is a supervised learning setting:\n",
    "We have input X and labels y.\n",
    "\"\"\"\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data      # shape: (569, 30)\n",
    "y = data.target    # shape: (569,)\n",
    "\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3) TRAIN / VALIDATION / TEST SPLIT\n",
    "# ==========================================================\n",
    "\n",
    "\"\"\"\n",
    "Why do we split?\n",
    "\n",
    "Training set   → used to learn parameters (W, b)\n",
    "Validation set → used to tune and detect overfitting\n",
    "Test set       → completely unseen data for final evaluation\n",
    "\n",
    "This is VERY important to avoid data leakage.\n",
    "\"\"\"\n",
    "\n",
    "# First split into training+validation and test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,          # 20% test\n",
    "    random_state=42,\n",
    "    stratify=y              # keeps class balance\n",
    ")\n",
    "\n",
    "# Now split training into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val,\n",
    "    test_size=0.2,          # 20% of training becomes validation\n",
    "    random_state=42,\n",
    "    stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Validation size:\", X_val.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 4) PREPROCESSING (FEATURE SCALING)\n",
    "# ==========================================================\n",
    "\n",
    "\"\"\"\n",
    "Neural networks are VERY sensitive to feature scale.\n",
    "\n",
    "If one feature ranges from 0–1000 and another from 0–1,\n",
    "gradients will behave badly and training becomes unstable.\n",
    "\n",
    "Therefore we standardize:\n",
    "\n",
    "x_scaled = (x - mean) / std\n",
    "\n",
    "Important:\n",
    "We FIT scaler ONLY on training data.\n",
    "Then TRANSFORM validation and test.\n",
    "This avoids data leakage.\n",
    "\"\"\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 5) CONVERT TO PYTORCH TENSORS\n",
    "# ==========================================================\n",
    "\n",
    "\"\"\"\n",
    "PyTorch works with tensors, not numpy arrays.\n",
    "\n",
    "Also:\n",
    "- Features → float32\n",
    "- Labels → float32 for BCELoss\n",
    "- Labels reshaped to (N,1) because output is (N,1)\n",
    "\"\"\"\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 6) DEFINE NEURAL NETWORK MODEL\n",
    "# ==========================================================\n",
    "\n",
    "\"\"\"\n",
    "From ML1 perspective:\n",
    "Logistic Regression model is:\n",
    "\n",
    "    y_hat = sigmoid(Wx + b)\n",
    "\n",
    "That is a SINGLE linear layer.\n",
    "\n",
    "Now we extend it using Deep Learning:\n",
    "\n",
    "    x → Linear → ReLU → Linear → ReLU → Linear → Sigmoid\n",
    "\n",
    "Why ReLU?\n",
    "Because without nonlinearity, stacking linear layers\n",
    "is still equivalent to one linear layer.\n",
    "\n",
    "ReLU allows learning nonlinear decision boundaries.\n",
    "\"\"\"\n",
    "\n",
    "class BreastCancerNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BreastCancerNN, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            # Input layer (30 features)\n",
    "            nn.Linear(30, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Hidden layer\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Output layer (1 neuron for binary classification)\n",
    "            nn.Linear(16, 1),\n",
    "            \n",
    "            # Sigmoid converts output to probability [0,1]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "model = BreastCancerNN()\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 7) DEFINE LOSS FUNCTION & OPTIMIZER\n",
    "# ==========================================================\n",
    "\n",
    "\"\"\"\n",
    "Since this is binary classification,\n",
    "we use Binary Cross Entropy:\n",
    "\n",
    "L = -[y log(y_hat) + (1-y) log(1-y_hat)]\n",
    "\n",
    "This is the SAME loss used in Logistic Regression.\n",
    "\n",
    "Optimizer:\n",
    "We use Adam instead of plain Gradient Descent.\n",
    "Adam adapts learning rate per parameter\n",
    "and converges faster in practice.\n",
    "\"\"\"\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 8) TRAINING LOOP\n",
    "# ==========================================================\n",
    "\n",
    "\"\"\"\n",
    "Training process:\n",
    "\n",
    "1) Forward pass → compute predictions\n",
    "2) Compute loss\n",
    "3) Backpropagation → compute gradients\n",
    "4) Update parameters\n",
    "\n",
    "We also compute validation loss\n",
    "to check if the model is overfitting.\n",
    "\"\"\"\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # -----------------\n",
    "    # TRAIN MODE\n",
    "    # -----------------\n",
    "    model.train()\n",
    "    \n",
    "    predictions = model(X_train)\n",
    "    train_loss = criterion(predictions, y_train)\n",
    "    \n",
    "    # Clear previous gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backpropagation\n",
    "    train_loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # -----------------\n",
    "    # VALIDATION MODE\n",
    "    # -----------------\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():   # disable gradient computation\n",
    "        val_predictions = model(X_val)\n",
    "        val_loss = criterion(val_predictions, y_val)\n",
    "    \n",
    "    \n",
    "    # Print progress every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "              f\"Train Loss: {train_loss.item():.4f} | \"\n",
    "              f\"Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 9) FINAL TEST EVALUATION\n",
    "# ==========================================================\n",
    "\n",
    "\"\"\"\n",
    "Now we evaluate on completely unseen test data.\n",
    "This gives the true generalization performance.\n",
    "\"\"\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    \n",
    "    # Convert probability → class label using threshold 0.5\n",
    "    test_predictions = (test_outputs > 0.5).float()\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(\"\\n========== FINAL TEST RESULTS ==========\")\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
